{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove columns that are irrelevant to the model\n",
    "data = data.drop(['Unnamed: 0', 'track_id', 'artists', 'album_name', 'track_name', 'popularity','Unnamed: 0'], axis=1)\n",
    "# remove time_signature, it's mostly 4/4 in this dataset\n",
    "data = data.drop(['time_signature'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode class targets\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "labEnc = LabelEncoder()\n",
    "y = data['track_genre']\n",
    "y = labEnc.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up traing and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "X = data.drop('track_genre', axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Standard Scaler to standardize the numerical features. It removes the mean and scales each feature/variable to unit variance.\n",
    "# z-score normalization\n",
    "# transfroms data to have mean of 0 and standard deviation of 1\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "continuous_features = ['duration_ms', 'danceability', 'energy', 'loudness', \n",
    "                       'speechiness', 'acousticness', 'instrumentalness', \n",
    "                       'liveness', 'valence', 'tempo' ]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train[continuous_features])\n",
    "X_train[continuous_features] = scaler.transform(X_train[continuous_features])\n",
    "X_test[continuous_features] = scaler.transform(X_test[continuous_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The categorical features are already encoded as integers. Explicit and mode are already coded to 0 and 1. Key is an integer between 1 and 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.008771929824561403, 0.008640350877192983)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# random guess accuracy\n",
    "genre_counts = 114\n",
    "from sklearn.dummy import DummyClassifier\n",
    "random_guessing_accuracy = 1 / genre_counts\n",
    "dummy_clf = DummyClassifier(strategy=\"stratified\", random_state=42)\n",
    "dummy_clf.fit(X_train, y_train)\n",
    "dummy_predictions = dummy_clf.predict(X_test)\n",
    "dummy_accuracy = accuracy_score(y_test, dummy_predictions)\n",
    "random_guessing_accuracy, dummy_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random guess accruacy of 0.88% is very low, which is expected due to the large number of genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1676315789473684"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Decision Tree Classifier Baseline Model\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "dt.fit(X_train, y_train)\n",
    "y_pred = dt.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree Classifier baseline model yielded an accuracy of 16.5%. Significantly better than random guess accuracy. Decent baseline to start with considering the simplicity of the model and large number of classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuned Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'min_samples_split': 70, 'max_depth': 40}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# search for best parameters with RandomizedSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "param_grid = {\n",
    "    'max_depth': list(range(10, 51, 10)),\n",
    "    'min_samples_split': list(range(10, 101, 20)),\n",
    "}\n",
    "random_search = RandomizedSearchCV(dt, param_grid, n_iter=25, cv=3, scoring='accuracy', n_jobs=-1, random_state=42)\n",
    "random_search.fit(X_train, y_train)\n",
    "best_params = random_search.best_params_\n",
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18140350877192982"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train and test with tuned parameters from RandomizedSearchCV\n",
    "dt = DecisionTreeClassifier(**best_params, random_state=42)\n",
    "dt.fit(X_train, y_train)\n",
    "y_pred = dt.predict(X_test)\n",
    "tuned_accuracy = accuracy_score(y_test, y_pred)\n",
    "tuned_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree Model with tuned paramters yielded an accuracy of 18.1%. This is small but I think significant increase in the accuracy over the baseline Decicion Tree model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP CLassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21653508771929825"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train a NN classifier for X to classify genre (y)\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# X_train_scaled = scaler.fit_transform(X_train)\n",
    "# X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100, 100), max_iter=100, random_state=42, early_stopping=True)\n",
    "# mlp = MLPClassifier(hidden_layer_sizes=(26, 26), max_iter=1000, random_state=42)\n",
    "mlp.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred = mlp.predict(X_test_scaled)\n",
    "mlp_accuracy = accuracy_score(y_test, y_pred)\n",
    "mlp_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nueral Network Model yielded an accuracy of 22.36%. This is an improvement over the tuned Decistion Tree model.  \n",
    "Next step is to tune the Nueral Network model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't execute this block, takes too long\n",
    "# Search for best parameters for NN model with GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(13,), (26,), (13, 13), (100, 100)],\n",
    "    'activation': ['relu', 'tanh', 'logistic'],\n",
    "    'solver': ['adam'],\n",
    "    'alpha': [0.0001, 0.01, 1],\n",
    "    'learning_rate': ['constant', 'adaptive'],\n",
    "}\n",
    "grid_search = GridSearchCV(mlp, param_grid, cv=3, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "best_params = grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GridSearchCV was stopped after an hour. Next we'll try RandomizedSearchCV with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of features in training data\n",
    "n_features = X_train.shape[1]\n",
    "n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mlp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 14\u001b[0m\n\u001b[0;32m      3\u001b[0m param_grid \u001b[39m=\u001b[39m {\n\u001b[0;32m      4\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mhidden_layer_sizes\u001b[39m\u001b[39m'\u001b[39m: [(\u001b[39m13\u001b[39m,), (\u001b[39m26\u001b[39m,), (\u001b[39m13\u001b[39m, \u001b[39m13\u001b[39m), (\u001b[39m26\u001b[39m, \u001b[39m26\u001b[39m)],\n\u001b[0;32m      5\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mactivation\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m'\u001b[39m\u001b[39mrelu\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtanh\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mlogistic\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m      6\u001b[0m     \u001b[39m'\u001b[39m\u001b[39msolver\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m      7\u001b[0m     \u001b[39m'\u001b[39m\u001b[39malpha\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m0.0001\u001b[39m, \u001b[39m0.01\u001b[39m, \u001b[39m1\u001b[39m],\n\u001b[0;32m      8\u001b[0m }\n\u001b[0;32m     10\u001b[0m \u001b[39m# subset_size = 10000\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[39m# X_train_subset = X_train_scaled[:subset_size]\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[39m# y_train_subset = y_train[:subset_size]\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m random_search \u001b[39m=\u001b[39m RandomizedSearchCV(mlp, param_grid, n_iter\u001b[39m=\u001b[39m\u001b[39m24\u001b[39m, cv\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, scoring\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m, n_jobs\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[39m# random_search.fit(X_train_subset, y_train_subset)\u001b[39;00m\n\u001b[0;32m     16\u001b[0m random_search\u001b[39m.\u001b[39mfit(X_train, y_train)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'mlp' is not defined"
     ]
    }
   ],
   "source": [
    "# Tune paramters with RandomizedSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(13,), (26,), (13, 13), (26, 26), (100, 100)],\n",
    "    'activation': ['relu', 'tanh', 'logistic'],\n",
    "    'solver': ['adam'],\n",
    "    'alpha': [0.0001, 0.01, 1],\n",
    "}\n",
    "\n",
    "# subset_size = 10000\n",
    "# X_train_subset = X_train_scaled[:subset_size]\n",
    "# y_train_subset = y_train[:subset_size]\n",
    "\n",
    "random_search = RandomizedSearchCV(mlp, param_grid, n_iter=24, cv=3, scoring='accuracy', n_jobs=-1)\n",
    "# random_search.fit(X_train_subset, y_train_subset)\n",
    "random_search.fit(X_train, y_train)\n",
    "best_params = random_search.best_params_\n",
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18350877192982457"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train and evaluate the classifier with the best parameters\n",
    "mlp_best = MLPClassifier(**best_params, max_iter=10000, random_state=42)\n",
    "mlp_best.fit(X_train_scaled, y_train)\n",
    "y_pred_best = mlp_best.predict(X_test_scaled)\n",
    "tuned_mlp_accuracy = accuracy_score(y_test, y_pred_best)\n",
    "tuned_mlp_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25049122807017543"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random Forest Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_clf = RandomForestClassifier(random_state=42)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "y_pred_rf = rf_clf.predict(X_test)\n",
    "rf_accuracy = accuracy_score(y_test, y_pred_rf)\n",
    "rf_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alec\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:425: FitFailedWarning: \n",
      "33 fits failed out of a total of 60.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "14 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Alec\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Alec\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 1144, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\Alec\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 637, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\Alec\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "19 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Alec\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Alec\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 1144, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\Alec\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 637, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\Alec\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\Alec\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:976: UserWarning: One or more of the test scores are non-finite: [0.24573684 0.245      0.24645614        nan        nan        nan\n",
      "        nan        nan 0.24885965 0.25024561 0.24496491 0.24768421\n",
      " 0.24936842        nan        nan        nan        nan        nan\n",
      " 0.24807018        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 200,\n",
       " 'min_samples_split': 5,\n",
       " 'min_samples_leaf': 1,\n",
       " 'max_features': 'sqrt',\n",
       " 'max_depth': 20}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tune parameters with RandomizedSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [5, 10, , 13, 100, 200, 500],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['auto', 'sqrt']\n",
    "}\n",
    "random_search_rf = RandomizedSearchCV(rf_clf, param_grid_rf, n_iter=20, cv=3, scoring='accuracy', n_jobs=-1, random_state=42, verbose=2)\n",
    "random_search_rf.fit(X_train, y_train)\n",
    "best_params_rf = random_search_rf.best_params_\n",
    "best_params_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25652631578947366"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tuned Random Forest Model\n",
    "rf_clf_best = RandomForestClassifier(**best_params_rf, random_state=42)\n",
    "rf_clf_best.fit(X_train, y_train)\n",
    "y_pred_rf_best = rf_clf_best.predict(X_test)\n",
    "tuned_rf_accuracy = accuracy_score(y_test, y_pred_rf_best)\n",
    "tuned_rf_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21618421052631578"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GradientBoostingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gb_clf = GradientBoostingClassifier(random_state=42)\n",
    "gb_clf.fit(X_train, y_train)\n",
    "y_pred_gb = gb_clf.predict(X_test)\n",
    "gb_accuracy = accuracy_score(y_test, y_pred_gb)\n",
    "gb_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alec\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Alec\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.14619298245614035"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SVM Model\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "svm_clf = svm.LinearSVC(random_state=42)\n",
    "svm_clf.fit(X_train_scaled, y_train)\n",
    "y_pred_svm = svm_clf.predict(X_test_scaled)\n",
    "svm_accuracy = accuracy_score(y_test, y_pred_svm)\n",
    "svm_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logisitic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alec\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.16791228070175437"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "log_reg_clf = LogisticRegression(random_state=42)\n",
    "log_reg_clf.fit(X_train, y_train)\n",
    "y_pred_log_reg = log_reg_clf.predict(X_test)\n",
    "log_reg_accuracy = accuracy_score(y_test, y_pred_log_reg)\n",
    "log_reg_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Logistic Regression model achieved an accuracy of approximately 16.03%, which is an improvement over the SVM model but still lower than the Random Forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16785964912280701"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LogisticRegression with higher max_iter\n",
    "log_reg_clf = LogisticRegression(max_iter=1000, random_state=42)\n",
    "log_reg_clf.fit(X_train_scaled, y_train)\n",
    "y_pred_log_reg = log_reg_clf.predict(X_test_scaled)\n",
    "log_reg_accuracy = accuracy_score(y_test, y_pred_log_reg)\n",
    "log_reg_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Stacking Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alec\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Alec\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Alec\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Alec\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Alec\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Alec\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Alec\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Alec\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Alec\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Alec\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Alec\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Alec\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Alec\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.2718421052631579"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "# base models\n",
    "level0 = list()\n",
    "level0.append(('lr', LogisticRegression(max_iter=100000)))\n",
    "level0.append(('rf', RandomForestClassifier(n_estimators=200, min_samples_split=2, min_samples_leaf=1, max_features='sqrt', max_depth=30)))\n",
    "level0.append(('svm', svm.LinearSVC()))\n",
    "\n",
    "# meta learner model\n",
    "level1 = LogisticRegression()\n",
    "stacking_model = StackingClassifier(estimators=level0, final_estimator=level1, cv=5)\n",
    "stacking_model.fit(X_train_scaled, y_train)\n",
    "y_pred_stacking = stacking_model.predict(X_test_scaled)\n",
    "stacking_accuracy = accuracy_score(y_test, y_pred_stacking)\n",
    "stacking_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearch parameter tuning for gradient boosting classifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 1],\n",
    "    'max_depth': [3, 5, 10]\n",
    "}\n",
    "gb_clf = GradientBoostingClassifier(random_state=42)\n",
    "grid_search = GridSearchCV(gb_clf, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_params = grid_search.best_params_\n",
    "best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopped gradient parameter tuning after 30+ minutes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Tune the hyperparameters of the models\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "param_dist_svm = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'loss': ['hinge', 'squared_hinge']\n",
    "}\n",
    "param_dist_gb = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 1],\n",
    "    'max_depth': [3, 5, 10]\n",
    "}\n",
    "\n",
    "svm = LinearSVC(max_iter=10000, random_state=42)\n",
    "gb = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "random_search_svm = RandomizedSearchCV(svm, param_dist_svm, n_iter=10, cv=5, scoring='accuracy', random_state=42)\n",
    "random_search_gb = RandomizedSearchCV(gb, param_dist_gb, n_iter=10, cv=5, scoring='accuracy', random_state=42)\n",
    "\n",
    "random_search_svm.fit(X_train_scaled, y_train)\n",
    "random_search_gb.fit(X_train, y_train)\n",
    "\n",
    "best_params_svm = random_search_svm.best_params_\n",
    "best_params_gb = random_search_gb.best_params_\n",
    "\n",
    "best_params_svm, best_params_gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "# base models\n",
    "level0 = list()\n",
    "level0.append(('lr', LogisticRegression(**best_params_lr)))   # assuming best_params_lr contains the best parameters for Logistic Regression\n",
    "level0.append(('rf', RandomForestClassifier(**best_params_rf)))  # assuming best_params_rf contains the best parameters for Random Forest\n",
    "level0.append(('svm', svm.LinearSVC(random_state=42)))  # assuming best_params_svm contains the best parameters for SVM\n",
    "level0.append(('gb', GradientBoostingClassifier(**best_params_gb)))  # assuming best_params_gb contains the best parameters for Gradient Boosting\n",
    "\n",
    "# meta learner model\n",
    "level1 = LogisticRegression()  # you can also tune the parameters for the meta learner\n",
    "stacking_model = StackingClassifier(estimators=level0, final_estimator=level1, cv=5)\n",
    "stacking_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict the test set results\n",
    "y_pred_stacking = stacking_model.predict(X_test_scaled)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "stacking_accuracy = accuracy_score(y_test, y_pred_stacking)\n",
    "stacking_accuracy\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
