{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove columns that are irrelevant to the model\n",
    "data = data.drop(['Unnamed: 0', 'track_id', 'artists', 'album_name', 'track_name', 'popularity','Unnamed: 0'], axis=1)\n",
    "# remove time_signature, it's mostly 4/4 in this dataset\n",
    "data = data.drop(['time_signature'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode class targets\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "labEnc = LabelEncoder()\n",
    "y = data['track_genre']\n",
    "y = labEnc.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up traing and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "X = data.drop('track_genre', axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Standard Scaler to standardize the numerical features. It removes the mean and scales each feature/variable to unit variance.\n",
    "# z-score normalization\n",
    "# transfroms data to have mean of 0 and standard deviation of 1\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "continuous_features = ['duration_ms', 'danceability', 'energy', 'loudness', \n",
    "                       'speechiness', 'acousticness', 'instrumentalness', \n",
    "                       'liveness', 'valence', 'tempo' ]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train[continuous_features])\n",
    "X_train[continuous_features] = scaler.transform(X_train[continuous_features])\n",
    "X_test[continuous_features] = scaler.transform(X_test[continuous_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The categorical features are already encoded as integers. Explicit and mode are already coded to 0 and 1. Key is an integer between 1 and 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.008771929824561403, 0.008640350877192983)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# random guess accuracy\n",
    "genre_counts = 114\n",
    "from sklearn.dummy import DummyClassifier\n",
    "random_guessing_accuracy = 1 / genre_counts\n",
    "dummy_clf = DummyClassifier(strategy=\"stratified\", random_state=42)\n",
    "dummy_clf.fit(X_train, y_train)\n",
    "dummy_predictions = dummy_clf.predict(X_test)\n",
    "dummy_accuracy = accuracy_score(y_test, dummy_predictions)\n",
    "random_guessing_accuracy, dummy_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random guess accruacy of 0.88% is very low, which is expected due to the large number of genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16758771929824562"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Decision Tree Classifier Baseline Model\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "dt.fit(X_train, y_train)\n",
    "y_pred = dt.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree Classifier baseline model yielded an accuracy of 16.5%. Significantly better than random guess accuracy. Decent baseline to start with considering the simplicity of the model and large number of classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuned Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'min_samples_split': 70, 'max_depth': 40}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# search for best parameters with RandomizedSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "param_grid = {\n",
    "    'max_depth': list(range(10, 51, 10)),\n",
    "    'min_samples_split': list(range(10, 101, 20)),\n",
    "}\n",
    "random_search = RandomizedSearchCV(dt, param_grid, n_iter=25, cv=3, scoring='accuracy', n_jobs=-1, random_state=42)\n",
    "random_search.fit(X_train, y_train)\n",
    "best_params = random_search.best_params_\n",
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18140350877192982"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train and test with tuned parameters from RandomizedSearchCV\n",
    "dt = DecisionTreeClassifier(**best_params, random_state=42)\n",
    "dt.fit(X_train, y_train)\n",
    "y_pred = dt.predict(X_test)\n",
    "tuned_accuracy = accuracy_score(y_test, y_pred)\n",
    "tuned_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree Model with tuned paramters yielded an accuracy of 18.1%. This is small but I think significant increase in the accuracy over the baseline Decicion Tree model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP CLassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1612719298245614"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train a NN classifier for X to classify genre (y)\n",
    "# one hidden layer with # of neurons = # of features\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# mlp = MLPClassifier(hidden_layer_sizes=(100, 100), max_iter=100, random_state=42, early_stopping=True)\n",
    "# mlp = MLPClassifier(hidden_layer_sizes=(26, 26), max_iter=1000, random_state=42)\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(13,), max_iter=100, random_state=42, early_stopping=True)\n",
    "mlp.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred = mlp.predict(X_test_scaled)\n",
    "mlp_accuracy = accuracy_score(y_test, y_pred)\n",
    "mlp_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16912280701754387"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# two hidden layers, each with # of neurons = # of features\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(13,13), max_iter=100, random_state=42, early_stopping=True)\n",
    "mlp.fit(X_train_scaled, y_train)\n",
    "y_pred = mlp.predict(X_test_scaled)\n",
    "mlp_accuracy = accuracy_score(y_test, y_pred)\n",
    "mlp_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17456140350877192"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# two hidden layers, 26 and 13 nodes each\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(26,13), max_iter=100, random_state=42, early_stopping=True)\n",
    "mlp.fit(X_train_scaled, y_train)\n",
    "y_pred = mlp.predict(X_test_scaled)\n",
    "mlp_accuracy = accuracy_score(y_test, y_pred)\n",
    "mlp_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19478070175438597"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# two hidden layers, 26 and 13 nodes each\n",
    "# larger # of iterations\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(26,13), max_iter=500, random_state=42)\n",
    "mlp.fit(X_train_scaled, y_train)\n",
    "y_pred = mlp.predict(X_test_scaled)\n",
    "mlp_accuracy = accuracy_score(y_test, y_pred)\n",
    "mlp_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19478070175438597"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# two hidden layers, 26 and 13 nodes each\n",
    "# larger # of iterations\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(26,13), max_iter=750, random_state=42)\n",
    "mlp.fit(X_train_scaled, y_train)\n",
    "y_pred = mlp.predict(X_test_scaled)\n",
    "mlp_accuracy = accuracy_score(y_test, y_pred)\n",
    "mlp_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21653508771929825"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# two hidden layers of 100 neurons each, and train for 500 epochs\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100,100), max_iter=500, random_state=42, early_stopping=True)\n",
    "mlp.fit(X_train_scaled, y_train)\n",
    "y_pred = mlp.predict(X_test_scaled)\n",
    "mlp_accuracy = accuracy_score(y_test, y_pred)\n",
    "mlp_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nueral Network Model yielded an accuracy of 22.36%. This is an improvement over the tuned Decistion Tree model.  \n",
    "Next step is to tune the Nueral Network model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't execute this block, takes too long\n",
    "# Search for best parameters for NN model with GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(13,), (13, 13), (26,13), (100, 100)],\n",
    "    'activation': ['relu', 'tanh', 'logistic'],\n",
    "    'solver': ['adam'],\n",
    "    'alpha': [0.0001, 0.01, 1],\n",
    "    'learning_rate': ['constant', 'adaptive'],\n",
    "}\n",
    "grid_search = GridSearchCV(mlp, param_grid, cv=3, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "best_params = grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GridSearchCV was stopped after an hour. Next we'll try RandomizedSearchCV with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of features in training data\n",
    "n_features = X_train.shape[1]\n",
    "n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'solver': 'adam',\n",
       " 'hidden_layer_sizes': (100, 100),\n",
       " 'alpha': 0.01,\n",
       " 'activation': 'logistic'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tune paramters with RandomizedSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(13,), (13, 13), (26,13), (100, 100)],\n",
    "    'activation': ['relu', 'tanh', 'logistic'],\n",
    "    'solver': ['adam'],\n",
    "    'alpha': [0.0001, 0.01, 1],\n",
    "}\n",
    "\n",
    "# subset_size = 10000\n",
    "# X_train_subset = X_train_scaled[:subset_size]\n",
    "# y_train_subset = y_train[:subset_size]\n",
    "\n",
    "mlp = MLPClassifier(max_iter=500, random_state=42)\n",
    "random_search = RandomizedSearchCV(mlp, param_grid, n_iter=24, cv=3, scoring='accuracy', n_jobs=-1)\n",
    "# random_search.fit(X_train_subset, y_train_subset)\n",
    "random_search.fit(X_train, y_train)\n",
    "best_params = random_search.best_params_\n",
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22416666666666665"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train and evaluate the classifier with the best parameters\n",
    "mlp_best = MLPClassifier(**best_params, max_iter=500, random_state=42)\n",
    "mlp_best.fit(X_train_scaled, y_train)\n",
    "y_pred_best = mlp_best.predict(X_test_scaled)\n",
    "tuned_mlp_accuracy = accuracy_score(y_test, y_pred_best)\n",
    "tuned_mlp_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2567982456140351"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random Forest Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_clf = RandomForestClassifier(random_state=42)\n",
    "rf_clf.fit(X_train_scaled, y_train)\n",
    "y_pred_rf = rf_clf.predict(X_test_scaled)\n",
    "rf_accuracy = accuracy_score(y_test, y_pred_rf)\n",
    "rf_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune parameters with RandomizedSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [5, 10, 13, 100, 200, 500],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['auto', 'sqrt']\n",
    "}\n",
    "random_search_rf = RandomizedSearchCV(rf_clf, param_grid_rf, n_iter=20, cv=3, scoring='accuracy', n_jobs=-1, random_state=42, verbose=2)\n",
    "random_search_rf.fit(X_test_scaled, y_train)\n",
    "best_params_rf = random_search_rf.best_params_\n",
    "best_params_rf\n",
    "# 9 min 49s runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 200,\n",
       " 'min_samples_split': 10,\n",
       " 'min_samples_leaf': 1,\n",
       " 'max_features': 'sqrt',\n",
       " 'max_depth': None}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params_rf = {'n_estimators': 200,\n",
    " 'min_samples_split': 10,\n",
    " 'min_samples_leaf': 1,\n",
    " 'max_features': 'sqrt',\n",
    " 'max_depth': None}\n",
    "best_params_rf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26035087719298244"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tuned Random Forest Model\n",
    "rf_clf_best = RandomForestClassifier(**best_params_rf, random_state=42)\n",
    "rf_clf_best.fit(X_test_scaled, y_train)\n",
    "y_pred_rf_best = rf_clf_best.predict(X_test_scaled)\n",
    "tuned_rf_accuracy = accuracy_score(y_test, y_pred_rf_best)\n",
    "tuned_rf_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GradientBoostingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gb_clf = GradientBoostingClassifier(random_state=42)\n",
    "gb_clf.fit(X_train_scaled, y_train)\n",
    "y_pred_gb = gb_clf.predict(X_test)\n",
    "gb_accuracy = accuracy_score(y_test, y_pred_gb)\n",
    "gb_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alec\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Alec\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.14619298245614035"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SVM Model\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "svm_clf = svm.LinearSVC(random_state=42)\n",
    "svm_clf.fit(X_train_scaled, y_train)\n",
    "y_pred_svm = svm_clf.predict(X_test_scaled)\n",
    "svm_accuracy = accuracy_score(y_test, y_pred_svm)\n",
    "svm_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logisitic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alec\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.16791228070175437"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "log_reg_clf = LogisticRegression(random_state=42)\n",
    "log_reg_clf.fit(X_train, y_train)\n",
    "y_pred_log_reg = log_reg_clf.predict(X_test)\n",
    "log_reg_accuracy = accuracy_score(y_test, y_pred_log_reg)\n",
    "log_reg_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Logistic Regression model achieved an accuracy of approximately 16.03%, which is an improvement over the SVM model but still lower than the Random Forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16785964912280701"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LogisticRegression with higher max_iter\n",
    "log_reg_clf = LogisticRegression(max_iter=1000, random_state=42)\n",
    "log_reg_clf.fit(X_train_scaled, y_train)\n",
    "y_pred_log_reg = log_reg_clf.predict(X_test_scaled)\n",
    "log_reg_accuracy = accuracy_score(y_test, y_pred_log_reg)\n",
    "log_reg_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Stacking Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alec\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Alec\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Alec\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Alec\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Alec\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Alec\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Alec\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Alec\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Alec\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Alec\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Alec\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Alec\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Alec\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.2718421052631579"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "# base models\n",
    "level0 = list()\n",
    "level0.append(('lr', LogisticRegression(max_iter=100000)))\n",
    "level0.append(('rf', RandomForestClassifier(n_estimators=200, min_samples_split=2, min_samples_leaf=1, max_features='sqrt', max_depth=30)))\n",
    "level0.append(('svm', svm.LinearSVC()))\n",
    "\n",
    "# meta learner model\n",
    "level1 = LogisticRegression()\n",
    "stacking_model = StackingClassifier(estimators=level0, final_estimator=level1, cv=5)\n",
    "stacking_model.fit(X_train_scaled, y_train)\n",
    "y_pred_stacking = stacking_model.predict(X_test_scaled)\n",
    "stacking_accuracy = accuracy_score(y_test, y_pred_stacking)\n",
    "stacking_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearch parameter tuning for gradient boosting classifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 1],\n",
    "    'max_depth': [3, 5, 10]\n",
    "}\n",
    "gb_clf = GradientBoostingClassifier(random_state=42)\n",
    "grid_search = GridSearchCV(gb_clf, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_params = grid_search.best_params_\n",
    "best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopped gradient parameter tuning after 30+ minutes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune the hyperparameters of the models\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "param_dist_svm = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'loss': ['hinge', 'squared_hinge']\n",
    "}\n",
    "# param_dist_gb = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 1],\n",
    "    'max_depth': [3, 5, 10]\n",
    "}\n",
    "svm = LinearSVC(max_iter=500, random_state=42)\n",
    "# gb = GradientBoostingClassifier(random_state=42)\n",
    "random_search_svm = RandomizedSearchCV(svm, param_dist_svm, n_iter=10, cv=5, scoring='accuracy', random_state=42)\n",
    "# random_search_gb = RandomizedSearchCV(gb, param_dist_gb, n_iter=10, cv=5, scoring='accuracy', random_state=42)\n",
    "random_search_svm.fit(X_train_scaled, y_train)\n",
    "# random_search_gb.fit(X_train, y_train)\n",
    "best_params_svm = random_search_svm.best_params_\n",
    "# best_params_gb = random_search_gb.best_params_\n",
    "best_params_svm, best_params_gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "# base models\n",
    "level0 = list()\n",
    "level0.append(('lr', LogisticRegression(**best_params_lr)))   # assuming best_params_lr contains the best parameters for Logistic Regression\n",
    "level0.append(('rf', RandomForestClassifier(**best_params_rf)))  # assuming best_params_rf contains the best parameters for Random Forest\n",
    "level0.append(('svm', svm.LinearSVC(random_state=42)))  # assuming best_params_svm contains the best parameters for SVM\n",
    "level0.append(('gb', GradientBoostingClassifier(**best_params_gb)))  # assuming best_params_gb contains the best parameters for Gradient Boosting\n",
    "\n",
    "# meta learner model\n",
    "level1 = LogisticRegression()  # you can also tune the parameters for the meta learner\n",
    "stacking_model = StackingClassifier(estimators=level0, final_estimator=level1, cv=5)\n",
    "stacking_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict the test set results\n",
    "y_pred_stacking = stacking_model.predict(X_test_scaled)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "stacking_accuracy = accuracy_score(y_test, y_pred_stacking)\n",
    "stacking_accuracy\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
